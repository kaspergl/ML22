{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# The Hello World of Supervised Learning with python and the sklearn package\n",
    "This exercise shows the high level interface to supervised learning using Python and Sklearn.\n",
    "\n",
    "Don't worry if you don't feel you have the full background knowledge yet. We will spend the next many weeks covering supervised learning. This exercise is just to show you a bit of what is coming up ahead.\n",
    "\n",
    "In supervised learning, we want to learn/approximate some unknown target function $f$ that maps data to whatever value we are interested in.\n",
    "To achieve this goal we are given a data set $D=\\{(x_1,y_1),\\dots,(x_n,y_n))\\}$ where $y_i = f(x_i)$.\n",
    "\n",
    "In these exercises, we will focus on classifaction tasks where $f(x_i)\\in \\{0,\\dots,k-1\\}$ for a problem with $k$ output classes.\n",
    "For instance for the problem of Spam Detection, f would be the function that takes as input an email and outputs whether it is spam or not spam.\n",
    "If we represent an email by a string, then $f$ is a mapping from strings to \\{0, 1\\}, 0 indicating not spam, and 1 indicating spam.\n",
    "To learn this function, we are given a list of mails (strings) and associated labels (spam, not spam)\n",
    "\n",
    "The very basic supervised learning approach is as follows.\n",
    "* Decide on a (parameterized) set of functions $H$ that we think can be used approximate $f$ on new data. \n",
    "* Use a *learning algorithm* on data $D$ to select a good $h\\in H$ that approximates $f$ well.\n",
    "* Apply h on new data that you care about about.\n",
    "\n",
    "The measure used to select $h \\in H$ by the learning algorithm, is defined over the data and designed by the machine learning engineer. It is supposed to encode what a good solution looks like, i.e. what function we prefer over others i.e. a function that minimize a relevant measure on the data, hopefully approximate $f$ well.\n",
    "\n",
    "For classification problems, a standard quality measure for a classifier is accuracy, i.e. what fraction of data points it manages to correctly classify/predict the label of.\n",
    "\n",
    "Enough theory talking. Let's demonstrate with some simple python code.\n",
    "\n",
    "# The Classifier Interface\n",
    "The function/learning algorithm $h$ is represented as an object that supports at least the methods **fit, predict, and score**.\n",
    "* fit(X, y), fit the data with learning algorithm on data X with labels y \n",
    "* predict(X), return the prediction of the fitted classifer on data points in X\n",
    "* score(X, y), compute the accuracy of the classifier on data X with labels y\n",
    "\n",
    "In general $X, y$ will be numpy arrays (a numpy array is a matrix/multidimensional container). With $n$ data points, $X$ is a matrix of shape $(n, d)$ where $d$ is the dimensionality of an input point and $y$ is a numpy array with shape $(n,)$.\n",
    "\n",
    "As a code example see next cell for the implementation of a very bad classifier.\n",
    "You should be able to figure out what this classifier does. Run the cell to see output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# first some magic functions you can ignore\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "class Classifier():\n",
    "    \"\"\" Dummy Classifier Class\"\"\"\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "        @param X, numpy array shape n,d (n input points of dimension d)\n",
    "        @return, numpy array shape n,  vector of predictions y same length as X\n",
    "        \"\"\"\n",
    "        pred = [self.state for x in X]\n",
    "        return np.array(pred)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Trains the classifier\n",
    "        Args:\n",
    "        -----\n",
    "        @param X, numpy array shape n,d (n input points of dimension d)\n",
    "        @param y, numpy array shape n,  vector of labels y same length as X\n",
    "        \"\"\"\n",
    "        self.state = y[0]\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\" Compute the accuracy of classifier on X, y \n",
    "        @param X, numpy array shape n,d (n input points of dimension d)\n",
    "        @param y, numpy array shape n,  vector of labels y same length as X\n",
    "        @return float, accuracy of trained classifier on data X, with labels y\n",
    "        \"\"\"\n",
    "        return np.mean(self.predict(X) == y)\n",
    "    \n",
    "c = Classifier()\n",
    "X = np.array([[2,2], [3,2]])\n",
    "y = np.array([0, 1])\n",
    "c.fit(X, y)\n",
    "pred = c.predict(X)\n",
    "print(pred)\n",
    "print('The accuracy is', c.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Using a Classifier from the Sklearn Package\n",
    "Let us experiment with the learning models already made by clever people instead of trying to improve the stupid classifier above.\n",
    "Below we have added some code for you to start with.\n",
    "There are two simple utility functions you can ignore, get_2D_data, and visualize2D that generate data, and plot data and classification results resepectively.\n",
    "\n",
    "\n",
    "The job is simply to use class the *Logistic Regression* model from the Sklearn package, which we will use for classification.  \n",
    "\n",
    "The gist of it is that an object of the class supports **fit, predict, score** for the machine learning/statistics algorithm known as Logistic Regression.\n",
    "For a long explanation, see http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Logistic Regression will be covered later in the course.\n",
    "\n",
    "\n",
    "## Exercise\n",
    "Complete the **apply_logreg** function by following the description. Run the cell so you can see the result and the visualized decision boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def get_2D_data():\n",
    "    \"\"\" Generate a data set to play with\"\"\"\n",
    "    X, y = make_classification(n_samples=500, n_features=2,\n",
    "                               n_redundant=0, n_informative=2,\n",
    "                               random_state=0, n_clusters_per_class=1)\n",
    "    return X, y\n",
    "\n",
    "def visualize2D(classifier, X, y):\n",
    "    \"\"\" Visualize a 2D classifer on data\"\"\"\n",
    "    h = 0.2\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.RdBu)\n",
    "    ax.contour(xx, yy, Z, [0], colors='k', linewidths=3)\n",
    "\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=1-y, cmap=plt.cm.Paired,\n",
    "               edgecolors='black', s=25)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title('Decision Boundary and Contour Plot - the stronger the color the more certain the prediction. \\\n",
    "                 The black line indicated the data points where the model think the label of the class is 50/50 percent')\n",
    "    plt.show()\n",
    "    \n",
    "def apply_logreg(feat, labels):\n",
    "    \"\"\" Run a simple (linear) classification model and see the results\n",
    "\n",
    "    Step 1. Create LogisticRegression classifier object\n",
    "    Step 2. Fit the data using the fit method\n",
    "    Step 3. Print the result (use score function on classfier ) on the training data\n",
    "    Step 4. Return the classifier object\n",
    "    \"\"\"\n",
    "    # knock yourself out\n",
    "    ### YOUR CODE around 4 lines of code\n",
    "    ### END CODE\n",
    "\n",
    "feat, labels = get_2D_data()\n",
    "logistic = apply_logreg(feat, labels)\n",
    "visualize2D(logistic, feat, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Let's try a real data set not in 2D\n",
    "We will take a classic machine learning data set about classifying wine.\n",
    "See https://archive.ics.uci.edu/ml/datasets/wine for info about the data.\n",
    "The data set now has 13 features so we cannot really plot in 2 dimensions, and we need to classify them into 3 classes.\n",
    "We progress nonetheless.\n",
    "\n",
    "Our goal is not necessarily to get a classifier that is accurate on the training data given to it. Instead, we want it to be accurate on new data. To allow us to measure this \"out-of-sample\" accuracy, we simply split the training data into two. A **training set** and a **test set**.\n",
    "\n",
    "We train the classifier on the training set and test the quality of the result on the test set. This loses data for training (bad!), but at least it gives us an estimate of the quality of our learned classification algorithm on new data that was not used for training. The test set can really only be used once, but in this exercise we cheat and use it at least three times.\n",
    "\n",
    "We will use three different learning models, the Logistic Regression from above, Decision trees, and the famous Neural Nets (all supporting fit, predict, score).\n",
    "* LogisticRegression:\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.decision_function\n",
    "* DecisionTrees -\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "* Neural Nets -\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "    May be hard to get good results out of the box! Dont lose to much sleep if the results are not to good.\n",
    "    \n",
    "We have three different methods below, one for each classifier type, for you to play with.\n",
    "To understand decision trees and what it has learned, you can use the visualize tree function that very nicely shows the classifier learned by the decision tree learning algorithm.\n",
    "It is also possible to try and peek at what the other two learning models learn, but is not as easily readable.\n",
    "\n",
    "\n",
    "You job is to fill the three training methods.\n",
    "\n",
    "All learning algorithms have multiple hyperparameters that influence the learning method. Since we have not covered any of them yet, you are only required to try the depth parameter for the trees since that one is fairly easy to understand.\n",
    "\n",
    "See the description in the underlying function.\n",
    "\n",
    "Before you knock yourself out, let's take a short look at the data\n",
    "\n",
    "## The Wine data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "from IPython.display import display    \n",
    "data = load_wine()\n",
    "print('Lets See the Features:')\n",
    "\n",
    "for name in data.feature_names:\n",
    "    print('-', name)    \n",
    "print('\\nFrom such data  you must specify one of three wines!!!')\n",
    "print('*'*30)\n",
    "print('Data Size:', data.target.shape[0], 'So Small!')\n",
    "print('Lets see the first 10 data points')\n",
    "print('*'*30)\n",
    "X = data.data\n",
    "y = data.target\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "display(df.head(10))\n",
    "print('*'*30)\n",
    "print('lets plot alcohol and magnesium colored with class')\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.scatter(X[:,0], X[:,4], c=y, cmap=plt.cm.Paired, s=20)\n",
    "plt.show()\n",
    "print('Not an obvious pattern - but some structure it seems - lets learn a classifier instead of fiddling with data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Learn to classify wine.\n",
    "Manually looking at this data and trying to implement a good rule for classifying the data is not fun, \n",
    "and this data set is in fact very small and simple...\n",
    "\n",
    "So machine learning please help.\n",
    "\n",
    "Your job is to apply machine learning to find a good classifier for wine using the wine data set.\n",
    "## Exercise\n",
    "* Complete train_wine_logistic and uncomment the test lines below\n",
    "* Complete train_wine_dectree and uncomment the test lines below. \n",
    "* Complete train_wine_neural_net and uncomment the test lines below. \n",
    "  The size of the network is determined by the hidden_layer_sizes parameter but you can use default values.\n",
    "\n",
    "Which methods seems best for this data set out of the box? (You can run it multiple time with different results due to randomness of the data split).\n",
    "Can you figure out which features are important for the classification for each model. Could it be important to know the *important* features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from io import StringIO  \n",
    "from IPython.display import Image, display\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def bar_plot_weights(weights, names):\n",
    "    \"\"\" Bar plot of weights\"\"\"\n",
    "    features = weights.shape[1]\n",
    "    classes = weights.shape[0]\n",
    "    fig, ax = plt.subplots(figsize=(13, 10))\n",
    "    index = np.arange(features)\n",
    "    bar_width = 0.15\n",
    "    opacity = 0.8\n",
    "    colors = ['r', 'g', 'b']\n",
    "\n",
    "    for i in range(classes):\n",
    "        rects2 = plt.bar(index + i * bar_width, weights[i, :], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     label='Wine {0}'.format(i))\n",
    " \n",
    "    ax.set_xlabel('feature')\n",
    "    ax.set_ylabel('feature weight')\n",
    "    ax.set_title('Wine Feature Weights')\n",
    "    ax.set_xticks(index + bar_width / 3, names)\n",
    "    ax.set_xticklabels(names)\n",
    "    plt.legend()\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_tree(dtree, feature_names):\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(dtree, out_file=dot_data,\n",
    "                    filled=True, rounded=True,\n",
    "                    special_characters=True, feature_names=feature_names)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "    img = Image(graph.create_png())\n",
    "    display(img)\n",
    "\n",
    "\n",
    "def print_score(classifier, X_train, X_test, y_train, y_test):\n",
    "    \"\"\" Simple print score function that prints train and test score of classifier - almost not worth it\"\"\"\n",
    "    print('In Sample Score: ',\n",
    "          classifier.score(X_train, y_train))\n",
    "    print('Test Score: ',\n",
    "          classifier.score(X_test, y_test))\n",
    "\n",
    "    \n",
    "def train_wine_logistic(X_train, y_train):\n",
    "    \"\"\"LogisticRegression as before -\n",
    "        http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.decision_function\n",
    "        return the trained logistic regression object\n",
    "        \n",
    "        For even this simple model there are many choices you can tune, \n",
    "        the most important parameter probably being  the C parameter which controls how \"complex\" we want the function. \n",
    "        The smaller C the simpler function. If you like try different value\n",
    "        \n",
    "        Step 1. Create LogisticRegression classifier object\n",
    "        Step 2. Fit the data using the fit method\n",
    "        Step 3. Return the Classifier\n",
    "    \"\"\"\n",
    "    # Knock your self out\n",
    "    ### YOUR CODE 3 lines\n",
    "    ### END CODE\n",
    "\n",
    "    \n",
    "def train_wine_dectree(X_train, y_train, max_depth=5):\n",
    "    \"\"\"\n",
    "    DecisionTrees -\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "    \n",
    "    For this DecisionTree model there are even kmore knobs to turn. But for this exercise you must set the max_depth parameter in the constructor of the classifier \n",
    "    and experiment with it a little. It will greatly affect how intricate a function you can learn.\n",
    "    \n",
    "    The parameter max_depth can be set in the constructor for the ClassificationTree object (as a named paramater) i.e. t = DecisionTreeClassifier(max_depth=42)\n",
    "    \n",
    "    Remember to return the tree.\n",
    "    \n",
    "        Step 1. Create DecisionTreeClassifier object\n",
    "        Step 2. Fit the data using the fit method\n",
    "        Step 3. Return the classifier \n",
    "    \"\"\"\n",
    "    # knock yourself out\n",
    "    ### YOUR CODE 3 lines\n",
    "    ### END CODE\n",
    "\n",
    "       \n",
    "def train_wine_neural_net(X_train, y_train):\n",
    "    \"\"\"\n",
    "   Neural Nets -\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "    Remember to set the hidden_layer_sizes in the construction i.e. MLPClassifier(hidden_layers_sizes=)\n",
    "    Otherwise train the MLPclassifer and return it/\n",
    "    \n",
    "    May be hard to get good results out of the box! Dont lose to much sleep if the results are not to good.\n",
    "    Setting other hyperparameters away from standard is probably required... \n",
    "    Set batch_size to a small number like 4 or 8 helps significantly\n",
    "    \n",
    "        Step 1. Create MPLClassifer  object\n",
    "        Step 2. Fit the data using the fit method on the data\n",
    "        Step 3. Return the classifier \n",
    "    \"\"\"\n",
    "    # knock yourself out\n",
    "    ### YOUR CODE 3 lines\n",
    "    ### END CODE\n",
    "\n",
    "# the main method here - comment out stuff you do not want to see\n",
    "## Get Data\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "\n",
    "## Try logistic regression\n",
    "head = '*'*20\n",
    "print(head, 'Logistic Regression Wine Score', head)\n",
    "#logistic = train_wine_logistic(X_train, y_train)\n",
    "#print_score(logistic, X_train, X_test, y_train, y_test)\n",
    "#print('Logistic Regreesion Learned Weights: ')\n",
    "#bar_plot_weights(logistic.coef_, data.feature_names)\n",
    "#print(data.feature_names)\n",
    "\n",
    "## try different max depth\n",
    "print('\\n', head,'Decision Tree Wine Score', head)\n",
    "#tree = train_wine_dectree(X_train, y_train, max_depth=5)\n",
    "#print_score(tree,  X_train, X_test, y_train, y_test)\n",
    "#print('Lets see the Tree')\n",
    "#plot_tree(tree, data.feature_names)\n",
    "\n",
    "## Try the famous neural nets\n",
    "#print('\\n', head, 'Neural Net Wine Score:', head)\n",
    "#mlp = train_wine_neural_net(X_train, y_train)\n",
    "#print_score(mlp, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
