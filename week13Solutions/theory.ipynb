{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 13 Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1: PCA on MNIST Digits\n",
    "In this exercise we will experiment with PCA on digits.\n",
    "\n",
    "1. Run PCA on the training data train_dat http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html (use n_components = 784) and use ``plot_images`` to plot the 20 directions with largest variance. Use this PCA model for all subsequent computations. The directions can be found with the attribute ``components_``.\n",
    "2. Take the first 20 data points from the training data and project them onto the first $k$ components for $k \\in \\{1, 2,4,8,16, 32, 64\\}$. Then plot them as images. What do you see?\n",
    "    **Hint:** To project them compute the length of the projection of each point onto the first $k$ directions and then compute for each image compute the linear combination of the first $k$ directions given by these directions ($XZZ^\\intercal$ as in lecture).\n",
    "3. Map all the training data train_dat to 2D (the length of the projection on the first two directions) and make a scatter plot where you color with the label and see if there is some structure (use scatter with ``cmap = plt.cm.Paired``, like ``ax.scatter(x,y, c=lab, cmap=plt.cm.Paired)``)\n",
    "4. Map all training data train_dat to $32$ dimensions and train an SGD Classifier. Observe the test accuracy of the classifier. How should you select target dimension in real life?\n",
    "Use the following classifer (svm loss): ``clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)``\n",
    "\n",
    "Discuss the results. Are they what you expect? Why? Why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## PCA on digits\n",
    "# In this exercise we will experiment with PCA on digits.\n",
    "\n",
    "# 1. Run PCA on the training data train_dat http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html (use n_components = 784) and use ``plot_images`` to plot the 20 directions with largest variance. Use this PCA model for all subsequent computations. The directions can be found with the attribute ``components_``.\n",
    "# 2. Take the first 20 data points from the training data and project them onto the first $k$ components for $k \\in \\{1, 2,4,8,16, 32, 64\\}$. Then plot them as images. What do you see?\n",
    "#     **Hint:** To project them compute the length of the projection of each point onto the first $k$ directions and then compute for each image compute the linear combination of the first $k$ directions given by these directions (XZZ^T as in lecture).\n",
    "# 3. Map all the training data train_dat to 2D (the length of the projection on the first two directions) and make a scatter plot where you color with the label and see if there is some structure (use scatter with ``cmap = plt.cm.Paired``, like ``ax.scatter(x,y, c=lab, cmap=plt.cm.Paired)``)\n",
    "# 4. Map all training data train_dat to $32$ dimensions and train an SGD Classifier. Observe the test accuracy of the classifier. How should you select target dimension in real life?\n",
    "#    Use the following classifer (svm loss): ``clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)``\n",
    "\n",
    "# Discuss the results. Are they what you expect? Why? Why not.\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Load full dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "\n",
    "\n",
    "\n",
    "def plot_images(dat, k=20, size=28):\n",
    "    \"\"\" Plot the first k vectors as 28 x 28 images \"\"\"\n",
    "    x2 = dat[0:k,:].reshape(-1, size, size)\n",
    "    x2 = x2.transpose(1, 0, 2)\n",
    "    fig, ax = plt.subplots(figsize=(20,12))\n",
    "    ax.imshow(x2.reshape(size, -1), cmap='bone')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    plt.show()\n",
    "\n",
    "#data, labels =\n",
    "data = mnist_trainset.data.numpy().reshape((60000, 28*28))\n",
    "labels = mnist_trainset.targets.numpy()\n",
    "\n",
    "# reduce size for speed\n",
    "rp = np.random.permutation(len(labels))\n",
    "train_dat = data[rp[:5000],:]\n",
    "test_dat = data[rp[5000:6000], :]\n",
    "train_lab = labels[rp[:5000]]\n",
    "test_lab = labels[rp[5000:6000]]\n",
    "\n",
    "components = None\n",
    "## TASK 1\n",
    "### YOUR CODE HERE\n",
    "pca = PCA(n_components=784)\n",
    "pca.fit(train_dat)\n",
    "components = pca.components_\n",
    "### END CODE\n",
    "print('First 20 directions (eigenvectors X^T X)')\n",
    "plot_images(components, 20)\n",
    "\n",
    "## TASK 2\n",
    "# take the first 20 data point and project them onto the first k components and plot for k in [1, 2,4,8,16, 32, 64]\n",
    "img = train_dat[0:20, :]\n",
    "print('Original images:')\n",
    "plot_images(img, 20)\n",
    "for k in [1, 2, 4, 8, 16, 32, 64]:\n",
    "    ### YOUR CODE HERE\n",
    "    print('{0}: compression visualization'.format(k))\n",
    "    Z = components[0:k].T\n",
    "    print('Z shape', Z.shape)\n",
    "    proj_imgs = img @ Z @ Z.T\n",
    "    print('pimgs shape', proj_imgs.shape)\n",
    "    plot_images(proj_imgs)\n",
    "    ### END CODE\n",
    "    \n",
    "# map the data to 2D and plot the results colored by label\n",
    "proj = None\n",
    "## TASK 3\n",
    "### YOUR CODE HERE\n",
    "proj = train_dat @ components[0:2].T\n",
    "### END CODE\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "ax.scatter(proj[:,0], proj[:,1], c=train_lab, cmap=plt.cm.Paired)\n",
    "plt.show()\n",
    "\n",
    "## TASK 4\n",
    "clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n",
    "### YOUR CODE HERE\n",
    "proj_train_dat = train_dat @ components[0:32].T\n",
    "clf.fit(proj_train_dat, train_lab)\n",
    "### END CODE\n",
    "\n",
    "clf_original = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n",
    "clf_original.fit(train_dat, train_lab)\n",
    "\n",
    "proj_test_dat = test_dat @ components[0:32].T\n",
    "acc = (clf.predict(proj_test_dat) == test_lab).mean()\n",
    "print('Test Accuracy of 32 Dim PCA:', 100*acc)\n",
    "print('Test Accuracy original:',100*((clf_original.predict(test_dat) == test_lab).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2: Dimensionality Reduction with Autoencoder\n",
    "\n",
    "Write an autoencoder in pytorch and use it to compute a dimensionality reduction for the MNIST digits (as used in PCA exercise) to $k$ numbers (hidden layer size). \n",
    "Then compare to PCA by plotting the reconstructed data points\n",
    "\n",
    "Technically you must complete the autoencoder class below.\n",
    "* Implement cost, encode and decode in the AutoEncoder class.      \n",
    "    For n data points with dimension d the cost is \n",
    "    $$\n",
    "    \\frac{1}{dn} \\sum_{i=1}^n \\sum_{j=1}^d (x_{i, j} - \\hat{x}_{i, j})^2 \n",
    "    $$\n",
    "    where $\\hat{x} = \\textrm{dec}(\\textrm{enc}(x))$ the result of encoding and then decoding $x$. Note that we normalize by $1/dn$ and not just $1/n$ like in the lecture. This is purely a practical choice that makes it easier to deal with learning rates.\n",
    "    Let $k$ denote the hidden layer size then:\n",
    "    - The encoder is given as  $\\textrm{enc}(x) = \\textrm{relu}(x W_1 + b_1)$, where $W_1$ is a matrix of size (784, k) and $b_1$ is of size (1, k)\n",
    "    - Similarly the decoder is $\\textrm{dec}(x) = (x W_2 + b_2)$ where $W_2$ is a matrix of size (k, 784) and $b_2$ is of size (1, 784)\n",
    "* Test the code by running simple_test and set hidden size to 16 and epochs to at least 10. It may take some time to actually fit. \n",
    "\n",
    "In the first cell we load the MNIST OCR data in pytorch way and show how to use the data iterators.\n",
    "The autoencoder class is in the next\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def plot_images(dat, k=16):\n",
    "    \"\"\" Plot the first k vectors as 28 x 28 images \"\"\"\n",
    "    size = 28 \n",
    "    x2 = dat[0:k,:].reshape(-1, size, size)\n",
    "    x2 = x2.transpose(1, 0, 2)\n",
    "    fig, ax = plt.subplots(figsize=(20, 12))\n",
    "    ax.imshow(x2.reshape(size, -1), cmap='bone')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "batch_size = 16\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=batch_size, shuffle=False)\n",
    "# works like this \n",
    "for idx, (X, y) in enumerate(train_loader):\n",
    "    x_vec = X.reshape(-1, 784)\n",
    "    print('Input Images')\n",
    "    plot_images(x_vec.numpy(), k=x_vec.shape[0]) # move to numpy only relevant when needing data in that format\n",
    "    if idx > 5:\n",
    "        break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AutoEncoder():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" The parameters required to be set by fit method \"\"\"\n",
    "        self.W1 = None\n",
    "        self.b1 = None\n",
    "        self.W2 = None\n",
    "        self.b2 = None\n",
    "\n",
    "    def cost(self, X, W1, b1, W2, b2):\n",
    "        \"\"\" Compute (coordinate-wise) Least Squares Loss of reconstructing the input.\n",
    "        The clamp function may be useful\n",
    "        \n",
    "          X: torch.tensor shape (n, d) - Data\n",
    "          W1: torch.tensor shape (d, h) - weights\n",
    "          b1: torch.tensor shape (1, h) - bias weight\n",
    "          W2: torch.tensor shape (h, d) - weights\n",
    "          b2: torch.tensor shape (1, d) - bias weight\n",
    "        returns pytorch tensor with least squared cost\n",
    "        \"\"\"\n",
    "   \n",
    "        loss = None\n",
    "        ### YOUR CODE HERE\n",
    "        enc_in = X @ W1 + b1\n",
    "        h_enc = enc_in.clamp(min=0)\n",
    "        decoded = h_enc @ W2 + b2 #shape n x d\n",
    "        assert decoded.shape == X.shape\n",
    "        squared_diff = (X - decoded)**2\n",
    "        errors = torch.mean(squared_diff, dim=1)\n",
    "        loss = torch.mean(errors)\n",
    "        ### END CODE\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, data_loader, hidden_size=32, epochs=5):   \n",
    "        \"\"\" GD Learning Algorithm with pytorch\n",
    "        \n",
    "         Args:\n",
    "         data_loader: torch dataloader allows enumeration over data\n",
    "         hidden_size: int\n",
    "         epochs: int \n",
    "         \n",
    "         sets \n",
    "        \"\"\"\n",
    "        def my_init(s_to, s_from):\n",
    "            \"\"\" Standard way to initialize matrices in neural nets - you can ignore it \"\"\"\n",
    "            w = torch.zeros(s_to, s_from)\n",
    "            b = torch.zeros(s_to, 1)\n",
    "            nn.init.kaiming_uniform_(w, a=np.sqrt(5))\n",
    "            bound = 1 / np.sqrt(s_from)\n",
    "            nn.init.uniform_(b, -bound, bound)\n",
    "            return torch.transpose(w, 1, 0), torch.transpose(b, 1, 0)        \n",
    "        W1, b1 = my_init(hidden_size, 784)\n",
    "        W2, b2 = my_init(784, hidden_size)\n",
    "        for i, z in enumerate([W1, b1, W2, b2]):\n",
    "            z.requires_grad_()\n",
    "\n",
    "        sgd = optim.SGD(params={W1, b1, W2, b2}, lr=0.1, weight_decay=1e-4)\n",
    "        #sgd = optim.AdamW(params={W1, b1, W2, b2}, lr=0.0001, weight_decay=1e-4)\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_count = 0\n",
    "            running_loss = 0\n",
    "            for idx, (X, y) in enumerate(data_loader):\n",
    "                sgd.zero_grad()\n",
    "                inputs = X.view(-1, 784) \n",
    "                loss = self.cost(inputs, W1, b1, W2, b2)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_count += 1\n",
    "                running_loss += loss.item()\n",
    "                if idx % 10 == 9:   \n",
    "                    print('Running loss: {2:.3f}'.format(epoch + 1, idx + 1,  epoch_loss/epoch_count), end='\\r')\n",
    "                    running_loss = 0.0\n",
    "\n",
    "                loss.backward()\n",
    "                sgd.step()\n",
    "            print('Epoch: {0}, Mean Least Square loss: {1}'.format(epoch + 1, epoch_loss/epoch_count))\n",
    "\n",
    "        self.W1 = W1.detach() #.numpy()\n",
    "        self.W2 = W2.detach() #.numpy()\n",
    "        self.b1 = b1.detach() #.numpy()\n",
    "        self.b2 = b2.detach() #.numpy()\n",
    "        \n",
    "\n",
    "    def encode(self, X):\n",
    "        \"\"\" Compute the embedded inputs.\n",
    "        \n",
    "        Args:\n",
    "         X: torch.tensor shape (n, d)\n",
    "         \n",
    "        Returns:\n",
    "         decoded: torch.tensor shape (n, h) using self.W1, self.b1 and ReLU\n",
    "        \"\"\"\n",
    "\n",
    "        encoded = None\n",
    "        ### YOUR CODE HERE\n",
    "        enc_in = X @ self.W1 + self.b1\n",
    "        encoded = enc_in.clamp(min=0)\n",
    "        ### END CODE\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, X):\n",
    "        \"\"\" Compute the reconstructed inputs from the encoding.\n",
    "        \n",
    "        Args:\n",
    "         X: torch.tensor shape (n, h)\n",
    "         \n",
    "        Returns:\n",
    "         decoded: torch.tensor shape (n, d) using self.W2, self.b2\n",
    "        \"\"\"\n",
    "\n",
    "        decoded = None\n",
    "        ### YOUR CODE HERE\n",
    "        decoded = X @ self.W2 + self.b2 #shape n x d\n",
    "        ### END CODE\n",
    "        return decoded\n",
    "    \n",
    "def simple_test(hidden_size=32, epochs=1):\n",
    "    net = AutoEncoder()\n",
    "    net.fit(data_loader=train_loader, hidden_size=hidden_size,  epochs=epochs)\n",
    "    X_sample, y_sample =  next(iter(train_loader))\n",
    "    print(X_sample.shape)\n",
    "    x_vec = X_sample.view(-1, 784)\n",
    "    with torch.no_grad():\n",
    "        reconstructed_sample = net.decode(net.encode(x_vec)).numpy()\n",
    "    print('Input Images')\n",
    "    plot_images(x_vec.numpy(), k=x_vec.shape[0])    \n",
    "    print('Reconstructed Images')\n",
    "    plot_images(reconstructed_sample, k=x_vec.shape[0])\n",
    "    \n",
    "    #data, labels =\n",
    "    data = dataset1.data.view(-1,784).type(torch.FloatTensor)\n",
    "    labels = dataset1.targets\n",
    "    # reduce size for speed\n",
    "    rp = np.random.permutation(len(labels))\n",
    "    train_dat = data[rp[:5000],:]\n",
    "    test_dat = data[rp[5000:6000], :]\n",
    "    train_lab = labels[rp[:5000]].numpy()\n",
    "    test_lab = labels[rp[5000:6000]].numpy()\n",
    "    \n",
    "    enc_train_dat = net.encode(train_dat)\n",
    "    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n",
    "    clf.fit(enc_train_dat, train_lab)\n",
    "    enc_test_dat = net.encode(test_dat)\n",
    "    acc = (clf.predict(enc_test_dat) == test_lab).mean()\n",
    "    print('AutoEncoder of dimension',hidden_size)\n",
    "    print('Test Accuracy using AutoEncoder:', 100*acc)\n",
    "    \n",
    "simple_test(32, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex3: Random Projections Experiment\n",
    "In the following, you must implement the random projection known as the Johnson-Lindenstrauss transform.\n",
    "\n",
    "The code you must implement:\n",
    "1. Fill out the random projection matrix Z such that a data matrix X can be embedded as XZ. Z should have the right scaling and normal distributed entries. Z should have k columns (the target dimension for embedding, called k in the lectures).\n",
    "2. Project the training data using Z\n",
    "3. Train the SGD classifier on the projected training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "# Load full dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "\n",
    "#data, labels =\n",
    "data = mnist_trainset.data.numpy().reshape((60000, 28*28))\n",
    "labels = mnist_trainset.targets.numpy()\n",
    "\n",
    "# reduce size for speed\n",
    "rp = np.random.permutation(len(labels))\n",
    "dat = data[rp[:6000], :]\n",
    "lab = labels[rp[:6000]]\n",
    "\n",
    "train_dat = dat[:5000,:]\n",
    "test_dat = dat[5000:, :]\n",
    "train_lab = lab[:5000]\n",
    "test_lab = lab[5000:]\n",
    "\n",
    "for k in [16, 32, 64, 128]:\n",
    "    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n",
    "    Z = None\n",
    "    ### YOUR CODE HERE\n",
    "    Z = np.random.normal(0,1.0/k,(28*28, k))\n",
    "    proj_dat = train_dat @ Z\n",
    "    clf.fit(proj_dat, train_lab)\n",
    "    ### END CODE\n",
    "\n",
    "    proj_test_dat = test_dat @ Z\n",
    "    acc = (clf.predict(proj_test_dat) == test_lab).mean()\n",
    "    print('Testing with target dimension',k)\n",
    "    print('Test accuracy of JL', 100*acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex4: Random Projections and Inner Products\n",
    "Recall that the many linear models we have seen in the course base their predictions on inner products $w^\\intercal x$. Moreover, recall from the lecture that the Johnson-Lindenstrauss transform guarantees (with high probability), for a set of feature vectors $x_1,\\dots,x_n \\in R^d$ that:\n",
    "$$\n",
    "\\|f(x_i) - f(x_j)\\|_2^2 \\in (1 \\pm \\varepsilon)\\|x_i - x_j\\|_2^2\n",
    "$$\n",
    "for all $x_i, x_j$. In this exercise, you must show that the Johnson-Lindenstrauss transform also preserves inner products with high probability. For this, we make an assumption that $-x_i$ is among $x_1,\\dots,x_n$ for every $x_i$.\n",
    "\n",
    "1. Assume $x_i$ and $x_j$ have unit length. Show that $|f(x_i)^\\intercal f(x_j) - x_i^\\intercal x_j| \\leq 2 \\varepsilon$ when all distances are preserved to within $(1\\pm \\varepsilon)$. Hint: The identity $4 \\langle a, b\\rangle = \\|a+b\\|_2^2 - \\|a-b\\|_2^2$ may be useful. Also, for two unit vectors $a,b$ it holds that $\\|a+b\\|_2^2 \\leq 4$ and $\\|a-b\\|_2^2 \\leq 4$.\n",
    "\n",
    "2. Assume the embedding $f$ is linear (like the construction in the lecture), i.e. $f(a v) = af(v)$ for constants $a$ and let $x_i$ and $x_j$ be arbitrary. Assume all distances are preserved to within $(1\\pm \\varepsilon)$. Show that $|f(x_i)^\\intercal f(x_j) - x_i^\\intercal x_j| \\leq 2 \\varepsilon \\|x_i\\|\\|x_j\\|$.\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION MATH\n",
    "\n",
    "1. Consider $4(f(x_i)^\\intercal f(x_j) - x_i^\\intercal x_j) = \\|f(x_i)+f(x_j)\\|_2^2 - \\|f(x_i)-f(x_j)\\|_2^2 - \\|x_i + x_j\\|_2^2 + \\|x_i - x_j\\|_2^2$.\n",
    "Since distances are preserved and $-x_i$ is in $x_1,\\dots,x_n$ for all $i$, this is in:\n",
    "$\\in (1 \\pm \\varepsilon)\\|x_i + x_j\\|_2^2 - (1 \\pm \\varepsilon)\\|x_i - x_j\\|_2^2 - \\|x_i + x_j\\|_2^2 + \\|x_i - x_j\\|_2^2 = \\pm \\varepsilon \\|x_i +x_j\\|_2^2 \\pm \\varepsilon \\|x_i - x_j\\|_2^2 \\subseteq \\pm 8 \\varepsilon$. Therefore, $|f(x_i)^\\intercal f(x_j) - x_i^\\intercal x_j| \\leq 2 \\varepsilon$.\n",
    "\n",
    "2. By linearity, we have $|f(x_i)^\\intercal f(x_j) - x_i^\\intercal x_j| = |\\|x_i\\| \\|x_j\\| f(x_i/\\|x_i\\|)^\\intercal f(x_j/\\|x_j\\|) - \\|x_i\\|\\|x_j\\| (x_i/\\|x_i\\|)^\\intercal (x_j/\\|x_j\\|)|$. Moving the constant $\\|x_i\\|\\|x_j\\|$ outside, we now have unit vectors and may apply the result from step 1. to conclude $|f(x_i)^\\intercal f(x_j) - x_i^\\intercal x_j| \\leq 2 \\varepsilon \\|x_i\\|\\|x_j\\|$.\n",
    "\n",
    "### END SOLUTION MATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex5: Random Projections and Support Vector Machines\n",
    "Recall from the support vector machines lectures that in the linearly separable case, we search for a hyperplane $w$ with the largest margin to the data. Assume there is such a hyperplane (let us ignore the bias $b$), specified by the normal vector $w$ of unit length, where the margin (geometric and functional since $\\|w\\| = 1$) to every training example is at least $\\gamma$. Assume furthermore that every training example $x_i$ has norm at most $R$.\n",
    "\n",
    "Show that a random projection into $k=C((R/\\gamma)^2 \\ln n)$ dimensions, for a big enough constant $C>0$, guarantees that the data is still linearly separable with high probability. Hint: It may be useful to use the properties proved in exercise 4 above and to use $\\varepsilon = \\gamma/(2R)$.\n",
    "\n",
    "We remark that since the VC-dimension of hyperplanes is $d+1$, the reduction to $k$ dimensions shows that a much smaller VC-dimension might suffice if there exists a separating hyperplane $w$ with large margins in $R^d$.\n",
    "\n",
    "### BEGIN SOLUTION MATH\n",
    "\n",
    "We know there is a unit vector $w$ such that $y_i w^\\intercal x_i \\geq \\gamma$ for all $(x_i,y_i)$. Add $w$ to $x_1,\\dots,x_n$ to form the set $X = \\{w,-x_1,x_1,\\dots,-x_n,x_n\\}$. Then with high probability, if $C$ is big enough, all inner products are preserved to within $\\pm (\\gamma/(2R)) \\|w\\|\\|x_i\\| = \\pm (\\gamma/(2R)) R = \\pm \\gamma/2$ by exercise 4. This means that $y_i f(w)^\\intercal f(x_i) \\geq y_i(x_i^\\intercal w \\pm \\gamma/2) = \\gamma - \\gamma/2 > 0$. So the data is still linearly separable. In particular, the vector $f(w)$ gives a separating hyperplane.\n",
    "\n",
    "### END SOLUTION MATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
