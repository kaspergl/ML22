{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Exercises\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1: Break Points and Growth Functions \n",
    "\n",
    "-   Is there always a break point for a finite hypothesis set of $n$\n",
    "    hypotheses? If so, can you give a an upper bound? What is the growth\n",
    "    function?\n",
    "\n",
    "-   Does the set of all functions have a break point? What is its growth\n",
    "    function?\n",
    "\n",
    "-   What is the (smallest) break point for the hypothesis set consisting\n",
    "    of circles centered around $(0,0)$? For a given circle the\n",
    "    hypothesis returns $1$ for points inside the circle and $-1$ for\n",
    "    points outside. What is the growth function?\n",
    "\n",
    "-   What if we move to centered balls in the 3-dimensional space\n",
    "    ${{\\mathbb R}}^3$? Or in general $d$-dimensional space\n",
    "    ${{\\mathbb R}}^d$ (hyperspheres)?\n",
    "\n",
    "-  Show that the growth function for a singleton hypothesis class $H = \\{h\\}$ is 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2: VC Dimension \n",
    "\n",
    "-   Does VC Dimension depend on the learning algorithm or the actual\n",
    "    data set given?\n",
    "\n",
    "-   Does VC Dimension depend on the probability distribution generating\n",
    "    the data (not the labels)?\n",
    "\n",
    "-   If $\\mathcal{H}_1 \\subseteq \\mathcal{H}_2$ is\n",
    "    $VC(\\mathcal{H}_1) \\leq VC(\\mathcal{H}_2)?$\n",
    "\n",
    "-   Can you give an upper bound on the VC dimension of a finite set of\n",
    "    $M$ hypotheses?\n",
    "\n",
    "-   What is the VC Dimension for the hypothesis set consisting of\n",
    "    circles centered around 0?\n",
    "\n",
    "-   What if we move to balls (3d)? or in general d dimensions\n",
    "    (hyperspheres)?\n",
    "\n",
    "-   What is the maximal VC dimension possible of the intersection of\n",
    "    hypothesis sets $\\mathcal{H}_1,\\dots,\\mathcal{H}_n$ with VC\n",
    "    dimension $v_1,\\dots,v_n$.\n",
    "\n",
    "-   As previous question, instead what is the minimal VC dimension of\n",
    "    the union of the hypothesis srets from the previous question\n",
    "\n",
    "-   Show that the VC dimension of the hypothesis set consisting of axis aligned rectangles in $\\mathbb{R}^2$ is 4,\n",
    "    i.e. find a point set of 4 points you can shatter and argue that any point set of size 5 can not.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3:  Book Exercise\n",
    "### Exercise 1.11 in the [LFD] Book \n",
    "(Not problems but exercises inside the text. page 25\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4: Regularization with Weight decay\n",
    "If we use weight decay regularization ($\\lambda||w||^2)$  for some real number $\\lambda$ in Linear Regression, what \n",
    "happens to the optimal weight vector if we let $\\lambda \\rightarrow \\infty$? (cost is $\\frac{1}{n} \\|Xw - y\\|^2 + \\lambda \\|w\\|^2$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 5: Grid Search For Regularization and Validation - Sklearn\n",
    "In this exercise we will optimize a [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) using regularization and validation.\n",
    "You must use the grid search module [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) from sklearn.\n",
    "\n",
    "In the cell below we have shown an example of how to use the grid search module to test two different values for max_depth for a decision tree for wine classification\n",
    "\n",
    "Your job is to find good hyperparameters for decision trees for breast cancer detection.\n",
    "\n",
    "### Task 1:\n",
    "For the breast cancer data set, find the best (or very good) combination of max_depth and min_samples_split (cell two below)\n",
    "\n",
    "The **max_depth** parameter controls the max depth of a tree and the deeper the tree the more complex the model.\n",
    "\n",
    "The **min_samples_split** controls how many elements the algorithm that constructs the tree is allowed to try and split.\n",
    "So if a subtree contains less than min_leaf_size elements, it many not be split into a larger subtree by the algorithm.\n",
    "\n",
    "\n",
    "### Task 2:\n",
    "- How long time does it take to use grid search validation for $k$ hyperparameters where we test each parameter for $d$ values, and the training algorithm uses $f(n)$ time to train on $n$ data points when we split the data into 5 parts.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine, load_breast_cancer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "def show_result(clf):\n",
    "    df = pd.DataFrame(clf.cv_results_)\n",
    "    df = df.sort_values('mean_test_score', ascending=False)\n",
    "    display(df)\n",
    "    print('best parameter found', clf.best_params_)\n",
    "    \n",
    "w_data = load_wine()\n",
    "wine_data = w_data.data\n",
    "wine_labels = w_data.target\n",
    "\n",
    "# grid search validation\n",
    "reg_parameters = {'max_depth': [1, 30]}  # dict with all parameters we need to test\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), reg_parameters, cv=3, return_train_score=True)\n",
    "clf.fit(wine_data, wine_labels)\n",
    "# code for showing the result\n",
    "bt = show_result(clf)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = load_breast_cancer()\n",
    "c_data = cancer_data.data\n",
    "c_labels = cancer_data.target\n",
    "\n",
    "\n",
    "def decisiontree_model_selection(train_data, labels):\n",
    "    clf = None\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "    return clf\n",
    "###\n",
    "clf = decisiontree_model_selection(c_data, c_labels)\n",
    "bt = show_result(clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 6: VC Dimension of Hyperplanes (Book Exercise 2.4 p. 52)\n",
    "Consider the input space $\\mathcal{X} = \\{1\\} \\times \\mathbb{R}^d$ (with the first coordinate being the constant 1). Show that the VC dimension of the hypothesis space $\\mathcal{H} = \\{\\textrm{sign}(w^\\intercal x) \\mid w\\in \\mathbb{R}^{d+1} \\}$ corresponding to the perceptron is $d+1$.\n",
    "\n",
    "We need to show \n",
    "1. That there exists a data set of size d+1 that can be shattered by hyperplanes\n",
    "2. That no data set of size d+2 can be shattered by hyperplanes\n",
    "\n",
    "We will give a few more hints than the book does.\n",
    "### Shattering d+1 points\n",
    "As the book hints you must create an \"easy\" data set that you store in matrix $X$. \n",
    "\n",
    "**Hint:** We suggest you consider as a data matrix, the $(d+1) \\times (d+1)$ matrix $X$ whose first column is all-1s (required since $\\mathcal{X} = \\{1\\} \\times \\mathbb{R}^d$) and where the lower $d \\times d$ corner of the matrix is the $d \\times d$ identity matrix.\n",
    "\n",
    "Show that you can construct any dichotomy $y \\in \\{-1,+1\\}^{d+1}$ using some $h \\in \\mathcal{H}$ and the data matrix $X$ defined above. That is, you have to show that for any $y \\in \\{-1,+1\\}^{d+1}$, you can find some hypothesis $w$ such that for all $i$, we have $\\textrm{sign}(w^\\intercal x_i)=y_i$ where $x_i$ is the $i$'th row of $X$.\n",
    "\n",
    "\n",
    "### No Shattering of d+2 points.\n",
    "Must show that for any d+2 points, there is a  dichotomy hyperplanes can not capture.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Consider an arbitrary set of d+2 points $x_1,\\dots, x_{d+2}$ of dimension (d+1) and think of them as vectors in $\\{1\\} \\times \\mathbb{R}^d \\subset \\mathbb{R}^{d+1}$.\n",
    "Since we have more vectors than dimensions the vectors must be linearly dependent.\n",
    "\n",
    "i.e. there is a $j$ such that:\n",
    "$$\n",
    "x_j = \\sum_{i\\neq j} a_i x_i\n",
    "$$\n",
    "Since $x_j$ is determined by the other data points then so is $w^\\intercal x_j$ for any $w$. This means the classification on point $x_j$ is dictated by the classification of the other data points and thus cannot freely be chosen.\n",
    "i.e.\n",
    "$$\n",
    "w^\\intercal x_j = w^\\intercal \\sum_{i\\neq j} a_i x_i =\\sum_{i\\neq j} a_i w^\\intercal x_i\n",
    "$$\n",
    "Define an impossible dichotomy as follows. \n",
    "$$\n",
    "y_i = \\textrm{sign}(a_i), \\quad i\\neq j, \\quad y_j = -1\n",
    "$$\n",
    "Show this dichotomy is impossible!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS Exercise If Time 7: Book Problem 2.18 In short\n",
    "Define\n",
    "$$\n",
    "\\mathcal{H}= \\{h_\\alpha \\mid h_\\alpha(x) = (-1)^{\\lfloor \\alpha\n",
    "          x\\rfloor}, \\alpha \\in {{\\mathbb R}}\\}\n",
    "$$ \n",
    "\n",
    "Show that the VC dimension of ${{\\mathcal H}}$ is infinite (even though there is only one parameter!)\n",
    "\n",
    "Hint: Use the points set\n",
    "$x_1=10,x_2=100,\\dots,x_i = 10^i,\\dots,x_N=10^N$ and show how to implement any dichotomy $y_1,\\dots,y_N \\in \\{-1, +1\\}^N$ (find $\\alpha$ that works).\n",
    "You can safely assume $\\alpha >0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
