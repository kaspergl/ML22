{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 14 Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1: TFIDF and Co-Occurrences\n",
    "Consider the following training set of 5 strings:\n",
    "\n",
    "d1 = \"machine learning is my favourite new topic\"\n",
    "\n",
    "d2 = \"i really like support vector machines\"\n",
    "\n",
    "d3 = \"gradient descent is a really neat algorithm\"\n",
    "\n",
    "d4 = \"the lecturer has no imagination with these strings\"\n",
    "\n",
    "d5 = \"enjoy the very last TA class\"\n",
    "\n",
    "* In the tf-idf representation of string d1, what is the coordinate/entry corresponding to the word \"is\"?\n",
    "\n",
    "* If we consider a context window of width 1 around every word in every string, how many co-occurrences are there between \"the\" and \"is\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2: TFIDF Using standard tools \n",
    "In this exercise you must implement basic text classification pipeline and apply it to SMS spam classification.\n",
    "The pipeline is\n",
    "\n",
    "* Transform strings to vectors using a library implementation of tf-idf.\n",
    "* Apply a standard Logistic Regression classifier on the training data\n",
    "  You need to complete the implementation of **run_tfidf_classifier**\n",
    "\n",
    "\n",
    "See here for a default implementation of transforming text to TFIDF transformed vectors\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "\n",
    "You may need to unzip the smsspamcollection file. For some reason, I need to run it twice after unzipping.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_data():\n",
    "    #wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "    with open('smsspamcollection/SMSSpamCollection', 'r') as f:\n",
    "        dat = f.readlines()\n",
    "    labels = [x.split('\\t')[0] for x in dat]\n",
    "    texts = [x.split('\\t')[1] for x in dat]\n",
    "    sl = set(labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.33, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def run_tfidf_classifier(train_strings, test_strings, train_labels, test_labels):\n",
    "    \"\"\" Transform strings to vectors and run a logistic regression model on it and see the results \n",
    "\n",
    "    You can use the following Logisticregression model (ask google for details if needed). Supports fit and score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    1. transform data, both train and test using TFIDF transform\n",
    "    2. fit Logistic Regression model\n",
    "    3. Print train score and test score\n",
    "    \"\"\"\n",
    "    print(train_strings[0:10])\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "\n",
    "\n",
    "run_tfidf_classifier(*get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex3: Analyzing Feature Hashing\n",
    "In feature hashing, we map a vector $x \\in R^d$ to a vector in $R^k$ using two hash functions $h : [d] \\to [k]$ and $g : [d] \\to \\{-1,1\\}$. The hash functions are chosen randomly and independently before seeing any data. Assume the hash functions satisfy the following two properties:\n",
    "1. For any two distinct coordinates $i \\neq j$, we have that $g(i)$ and $g(j)$ are independent and uniform random, i.e. for any $a,b \\in \\{-1,1\\}$ it holds that $\\Pr_g[g(i)=a \\wedge g(j)=b] = 1/4$.\n",
    "2. For any two distinct coordinates $i \\neq j$, we have that $\\Pr_h[h(i)=h(j)] \\leq 1/k$.\n",
    "\n",
    "The embedding $f(x)$ of a vector $x$ is obtained by hashing each index $i \\in [d]$ to the index $h(i)$ and adding $g(i) \\cdot x_i$ to $f(x)_{h(i)}$.\n",
    "\n",
    "Your task it to prove:\n",
    "1. For two vectors $x,y$, we have $\\mathbb{E}[f(x)^\\intercal f(y)] = x^\\intercal y$.\n",
    "\n",
    "Hint: The following re-writing may be useful:\n",
    "$$\n",
    "f(x)^\\intercal f(y) = \\sum_{i=1}^d \\sum_{j=1}^d 1_{[h(i)=h(j)]} x_i y_j g(i) g(j),\n",
    "$$\n",
    "where $1_{[h(i)=h(j)]}$ is the indicator random variable taking the value $1$ if $h(i)=h(j)$ and $0$ otherwise.\n",
    "You may also need linearity of expectation $\\mathbb{E}[A + B] = \\mathbb{E}[A] + \\mathbb{E}[B]$ and that for independent random variables $X,Y$ we have $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex4: Skip-Gram\n",
    "This exercise can be found in the separate notebook skipgram.ipynb. We recommend uploading the notebook to Google Colab for faster execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
