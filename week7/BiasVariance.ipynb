{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Bias Variance Tradeoff\n",
    "### Step 1: Fitting and Plotting\n",
    "\n",
    "In this exercise we fit cos(x) using a few  data points **sampled indepently and uniformly at random** from $[0,2\\pi]$ using the following two models (using least squares error).\n",
    "- model 1: m1 - fit the best linear model to the data $(w_1 x + w_0)$\n",
    "- model 2: m2 - fit the best second order polynomial of the the data $(w_2x^2 +w_1x + w_0)$\n",
    "\n",
    "    \n",
    "We have already a large part of the code so we can get to the main point without being bogged down with python intricacies. \n",
    "\n",
    "You need to to write the code that takes a random data sample  (np.array of shape (datasize, ))\n",
    "and create training data that works for model m1 and m2 respectively and fit the two models using least squares cost (Linear Regression)\n",
    "    \n",
    "**Task:**\n",
    "In the code cell below you need to make/compute/fill**\n",
    "- train_set_m1 - input data to model m1 (np.array shape: (datasize, 2) (why?)\n",
    "- train_set_m2 - input data to model m2 (np.array shape: (datasize, 3) (why?)\n",
    "- train_target - target data (should be (cos(x))) \n",
    "- w_opt_m1 - learned parameters using model 1 on data train_set_m1 and target train_target\n",
    "- w_opt_m2 - learned parameters using model 2 on data train_set_m2 and target train_taget\n",
    "\n",
    "Run the next cell to see the results.\n",
    "    \n",
    "HINT: Look at the code for making test sets for train_set_m1 and train_set_m2\n",
    "HINT: la.pinv may come in handy for computing w_opt_m1 and w_opt_m2\n",
    "\n",
    "**Task:** Try changing the datasize parameter. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import numpy.random as rd\n",
    "\n",
    "\n",
    "\n",
    "def intro(datasize=3,  examples=(4, 4)):\n",
    "    \"\"\" \n",
    "    Fit cos(x) and plot on data size <data_size> randomly chosen from 0 -> 2pi \n",
    "    using models:\n",
    "    model1: m1 fit the best line between the points (ax + b)\n",
    "    model2: m2 fit the best second order polynomial of the the data (ax^2 +bx + c)\n",
    "    \n",
    "    make a grid of size <examples> and plot examples[0] * examples[1] \n",
    "    \n",
    "    We have written most of the code. \n",
    "    You need to to write the code that takes a random data sample of size data size (np.array of shape (datasize, ))\n",
    "    and make training data that works for model1 and model2 and fit the two models (least squares - hint linear regression)\n",
    "    \n",
    "    You need to make compute/fill\n",
    "    - train_set_m1 - input data to model m1 (np.array shape: (datasize, 2) (why?)\n",
    "    - train_set_m2 - input data to model m2 (np.array shape: (datasize, 3) (why?)\n",
    "    - train_target - target data (should be cos(x)) \n",
    "    - w_opt_m1 - learned parameters using model 1 on data train_set_m1 and target train_target\n",
    "    - w_opt_m2 - learned parameters using model 2 on data train_set_m2 and target train_taget\n",
    "\n",
    "    \n",
    "    HINT: Look at the code for making test sets for train_set_m1 and train_set_m2\n",
    "    HINT: la.pinv may come in handy for computing w_opt_m1 and w_opt_m2\n",
    "    \"\"\"\n",
    "    data_range = 2*np.pi\n",
    "    plot_points = 1000\n",
    "    x_data = np.linspace(0, data_range, plot_points)\n",
    "    \n",
    "    # container for test sets\n",
    "    test_set_m1 = np.ones((plot_points, 2))\n",
    "    test_set_m1[:,1] = x_data\n",
    "    test_set_m2 = np.ones((plot_points, 3))\n",
    "    test_set_m2[:,1] = x_data\n",
    "    test_set_m2[:,2] = x_data **2\n",
    "    \n",
    "    # figures and axes to plot on\n",
    "    fig, axes = plt.subplots(examples[0], examples[1], figsize=(20, 16))\n",
    "    for i in range(examples[0]):\n",
    "        for j in range(examples[1]):\n",
    "            ax = axes[i][j]\n",
    "            train_dat = rd.rand(datasize)*data_range\n",
    "            w_opt_m1 = None\n",
    "            w_opt_m2 = None\n",
    "            train_target = None\n",
    "            train_set_m1 = np.ones((datasize, 2))\n",
    "            train_set_m2 = np.ones((datasize, 3))\n",
    "            ### YOUR CODE HERE Compute and set w_opt_m1/m2, train_target and train_set_m1/m2\n",
    "            ### END CODE\n",
    "            ax.plot(x_data, np.dot(test_set_m1, w_opt_m1), 'b-', linewidth=2, label='line') # plot model \n",
    "            ax.plot(x_data, np.dot(test_set_m2, w_opt_m2),'g-',linewidth=2, label='2 order poly') # plot model 2\n",
    "            ax.plot(x_data, np.cos(x_data), 'r-', linewidth=2, label='target') # plot the target function\n",
    "            ax.plot(train_dat, train_target, 'ks', markersize=6) # plot the sampled data set\n",
    "            ax.legend()\n",
    "            ax.set_xlim(0, data_range)\n",
    "            ax.set_ylim(-3, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "intro()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Experimentally evaluate the bias variance tradeoff for models used above\n",
    "         \n",
    "The setup and models are as above, line and polynomial fitting $\\cos(x)$ on $[0, 2\\pi]$ using a few  data points.\n",
    "Now your task is to experimentally evaluate the Bias Variance tradeoff using  sampling.\n",
    "    \n",
    "**Task:** \n",
    "The steps are:\n",
    "1. repeat fits_to_plot times\n",
    "  - Generate random data - as above\n",
    "  - Fit the two models - as above\n",
    "  - plot the two models, blue for line, green for polynomial - as above except all on same plot/axes\n",
    "  - compute $E_\\textrm{in}$ and $E_\\textrm{out}$ for the two fitted models ($E_\\textrm{out}$ is estimated by sampling) and accumulate\n",
    "    \n",
    "2. Compute the average $E_\\textrm{in}$ and $E_\\textrm{out}$ for the two models using the accumulated values - print to screen\n",
    "3. Compute the average model for m1, m2 - print to screen, plot to axes\n",
    "4. Compute/Estimate the bias variance tradeoff for the two models - print to screen - see if it makes sense (only need to compute bias since we have estimated  $E_\\textrm{out}$  already)\n",
    "    \n",
    "We have written some of the code so you have to fill in the rest. \n",
    "    \n",
    "HINT: Look at the code above\n",
    "HINT: la.pinv may come in handy for computing w_opt_m1 and w_opt_m2\n",
    "HINT: To compute bias variance tradeoff when we know eout, all you need to know is the bias which is \"easy\" to estimate from the average model and the test data\n",
    "    \n",
    "    \n",
    "Try with datasize 2, 3, 5, 10 what do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_var_exp(datasize=3, fits_to_plot=2000):\n",
    "    \"\"\" Experimentally evaluate the bias variance tradeoff for models used above\n",
    "         \n",
    "    The models are as above, line and polynomial fitting cos on 0, 2pi using <data_size> data points.\n",
    "    Now we need to experimentally evaluate bias variance tradeoff using standard sampling.\n",
    "    \n",
    "    The steps are:\n",
    "    repeast fits_to_plot times\n",
    "     - Generate random data - as above\n",
    "     - Fit the two models - as above\n",
    "     - plot the two models, blue for line, green for polynomial - as above\n",
    "     - compute ein and eout for the two fitted models (eout is estimated by sampling) and accumulate\n",
    "    \n",
    "    Compute the average ein and eout for the two models - print to screen\n",
    "    Compute the average model for m1, m2 - print to screen, plot to axes\n",
    "    Compute/Estimate the bias variance tradeoff for the two models - print to screen -\n",
    "    see if it makes sense (only compute bias since we have estimated eout already)\n",
    "    \n",
    "    We have written some of the code. \n",
    "    \n",
    "    \n",
    "    HINT: Look at the code above\n",
    "    HINT: la.pinv may come in handy for computing w_opt_m1 and w_opt_m2\n",
    "    HINT: To compute bias variance tradeoff when we know eout, all you need to know is the bias which is \"easy\" to \n",
    "    estimate from the average model and the test data\n",
    "    \"\"\"\n",
    "    print('Bias Variance Experiment: Lines vs 2D Polynomial for fitting cos(x) on [0-2pi]')\n",
    "    data_range = 2*np.pi\n",
    "    plot_points = 1000\n",
    "    x_data = np.linspace(0, data_range, plot_points)\n",
    "    \n",
    "    # containers for test set\n",
    "    test_set_m1 = np.ones((plot_points, 2))\n",
    "    test_set_m1[:,1] = x_data\n",
    "    test_set_m2 = np.ones((plot_points, 3))\n",
    "    test_set_m2[:,1] = x_data\n",
    "    test_set_m2[:,2] = x_data **2\n",
    "    test_target = np.cos(x_data)\n",
    "\n",
    "    # figure and axes to plot on\n",
    "    fig, ax = plt.subplots(figsize=(20, 16))\n",
    "\n",
    "    # Accumulatation variables\n",
    "    ein_m1=0  # accumulated ein for model 1\n",
    "    eout_m1=0  # accumulated eout for model 2\n",
    "    ein_m2=0  # accumulated ein for model 1\n",
    "    eout_m2=0  # accumulated eout for model 2\n",
    "    \n",
    "    # containers for average model\n",
    "    w1_bar = np.zeros((2,))\n",
    "    w2_bar = np.zeros((3,))\n",
    "    for i in range(1, fits_to_plot):\n",
    "        train_dat = rd.rand(datasize)*data_range\n",
    "        train_set_m1 = np.ones((datasize, 2))\n",
    "        train_set_m2 = np.ones((datasize, 3))\n",
    "        w_opt_m1 = None\n",
    "        w_opt_m2 = None\n",
    "        train_target = None\n",
    "        ### YOUR CODE HERE - compute train_set_m1, train_set_m2, target, w_opt_m1, w_opt_m2\n",
    "        ### END CODE\n",
    "        #accumulate average model parameters\n",
    "        w1_bar += w_opt_m1/fits_to_plot\n",
    "        w2_bar += w_opt_m2/fits_to_plot\n",
    "\n",
    "        # Compute in sample errors (on train_set_m1/m2 and train_target)\n",
    "        ein_this_m1 = 0\n",
    "        ein_this_m2 = 0\n",
    "        ### YOUR CODE HERE set ein_this_m1/m2     \n",
    "        ### END CODE\n",
    "        # accumulate eins\n",
    "        ein_m1 += ein_this_m1/fits_to_plot\n",
    "        ein_m2 += ein_this_m2/fits_to_plot\n",
    "        \n",
    "        # Estimate eout\n",
    "        eout_this_m1 = 0\n",
    "        eout_this_m2 = 0\n",
    "        h1 = np.dot(test_set_m1, w_opt_m1)\n",
    "        h2 = np.dot(test_set_m2, w_opt_m2)      \n",
    "        ### YOUR CODE HERE - Set eout_this_m1/m2 by estimating eout using test data (h1/h2, test_target) for m1 and m2.\n",
    "        ### END CODE\n",
    "        # accumulate eouts\n",
    "        eout_m1 += eout_this_m1/fits_to_plot\n",
    "        eout_m2 += eout_this_m2/fits_to_plot\n",
    "        \n",
    "        # plotting the results\n",
    "        ax.plot(x_data, h1, 'b-', linewidth=0.5)\n",
    "        ax.plot(x_data, h2, 'g-', linewidth=0.5)\n",
    "\n",
    "    framer = '*'*10\n",
    "    print('\\n', framer, 'Model In Sample and out of sample Error', framer)\n",
    "    print('Model 1 Ein: {0} - Eout: {1}'.format(ein_m1, eout_m1))\n",
    "    print('Model 2 Ein: {0} - Eout: {1}'.format(ein_m2, eout_m2))\n",
    "\n",
    "    print('\\n', framer, 'Average Models Found:', framer)\n",
    "    print('Average Model M1:', w1_bar)\n",
    "    print('Average Model M2:', w2_bar)\n",
    "    \n",
    "    bias_m1 = 0\n",
    "    var_m1 = 0\n",
    "    bias_m2 = 0\n",
    "    var_m2 = 0    \n",
    "    # Use test_set_m1/m2 and test_target to estimate biases. Use the average model parameters\n",
    "    # w1_bar and w2_bar computed above\n",
    "    ### YOUR CODE HERE - COMPUTE ESTIMATED BIAS AND VARIANCE assign bias_m1, bias_m2\n",
    "    ### END CODE\n",
    "    #Hack for obtaining variance from eout and bias\n",
    "    var_m1 = eout_m1 - bias_m1 \n",
    "    var_m2 = eout_m2 - bias_m2 \n",
    "    print('\\n', framer, 'Bias Variance Tradeoff Estimated', framer)\n",
    "    print('Model 1 bias variance {0} - {1}'.format(bias_m1, var_m1))\n",
    "    print('Model 2 bias variance {0} - {1}'.format(bias_m2, var_m2))\n",
    "    \n",
    "    # Plot Average models Vs Target Model\n",
    "    ax.plot(x_data, test_target,'r-',linewidth=3, label='target')\n",
    "    ax.plot(x_data, np.dot(test_set_m1, w1_bar), 'm-', linewidth=4, label='Average Line (blue)')\n",
    "    ax.plot(x_data, np.dot(test_set_m2, w2_bar), 'y-', linewidth=4, label='Average 2. order (green)')\n",
    "    ax.set_ylim([-3., 3.])\n",
    "    ax.set_xlim(0., data_range)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bias_var_exp(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
