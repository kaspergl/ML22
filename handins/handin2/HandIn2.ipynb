{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets for Multiclass Classification\n",
    "In this exercise you must implement a multi-class classifier based on neural nets.\n",
    "\n",
    "You must implement a one hidden layer neural net with relu activation function for the hidden layer and $K$ outputs that are not transformed (i.e.) the output neurons compute the identity function.\n",
    "\n",
    "As the loss function, the $K$ outputs of the neural net are transformed by softmax and the loss is mean negative log likelihood (special case of cross entropy loss, and it is the same loss as considered in Hand In 1) and is explained in the next cell.\n",
    "\n",
    "For combatting overfitting you must implement weight decay and use validation to output the best model parameters seen during training.\n",
    "\n",
    "With weight decay the cost of one data point becomes becomes\n",
    "\n",
    "$$\n",
    "-\\sum_{i=1}^k y_i \\ln \\textrm{softmax}(\\textrm{nn}(x))_i + \\lambda \\left(\\sum_{i,j} W_1[i,j]^2 + \\sum_{i, j} W_2[i, j]^2\\right)\n",
    "$$\n",
    "\n",
    "where $y$ is the one-in-k encoding of the data point label (one entry with a one, the rest are zero), nn(x) is the output of the neural net on data point $x$, $\\textrm{softmax}(\\textrm{nn}(x))_i$ is the $i$'th entry in the vector of *probabilities* computed by softmax, and $W_1$, and $W_2$ are the weight matrices for the neural net. Note that biases are not part of the weight decay term.\n",
    "\n",
    "For validation, you must evaluate the accuracy on a validation set after each epoch (note this does not include the weight decay term or the nll term, only accuracy). The final returned model must be the one obtaining the best validation accuracy.\n",
    "\n",
    "\n",
    "## Your task is\n",
    "- Derive a very useful derivative described in the next cell\n",
    "- Complete the neural net class in net_classifier.py.\n",
    "- Test your implementation using net_test.py\n",
    "\n",
    "The prerequisites for the report is listed last.\n",
    "\n",
    "**Solve the derivative exercise in the next cell (derivation must be included in report) before you start the implementation shortly described in the following cell.**\n",
    "\n",
    "\n",
    "## Backpropagation Help\n",
    "As discussed in lectures, a nice way to understand backpropagation is given  [here](http://cs231n.github.io/optimization-2/).\n",
    "Everything can be coded in a vectorized way using matrices and vectors and their products. But this is **not** required.\n",
    "For example, it is fine to implement the derivative computation assuming just one input point, and then repeating that computation for each input data point in the mini-batch.\n",
    "\n",
    "The test in net_classifier.py tests the gradient of your cost using a numerical estimate of the gradient of the cost for every parameter.\n",
    "\n",
    "**Start Early, Use The Study Cafe, Check Your Shapes/Dimensions, know there may be numerial issues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives of Negative Log Likelihood as a function of the input to Softmax (the output of the neural net)\n",
    "In this exercise you must compute the derivative of the softmax operation and the negative log likelihood error in one step as this is much simpler than computing them step by step and  combining them with the chain rule. \n",
    "\n",
    "Let $z$ be the $k$-dimensional input to softmax and let $y$ denote the one-hot encoded vector of labels for a given data point (we assume $y$ has a single $1$ and the rest of the entries are $0$).\n",
    "As a function of $z$, the Negative Log Likelihood loss for $z$ when the true label is $j \\in \\{1,\\dots,k\\}$ is\n",
    "$$\n",
    "L(z) =  - \\sum_{i=1}^k y_i \\ln (\\textrm{softmax}(z)_i) = - \\ln (\\textrm{softmax}(z)_j) \n",
    "$$\n",
    "\n",
    "As a function of $z$, the cost $L$ is a function that maps $K$ numbers to one. We are looking for the partial derivative of $L$ relative to every entry in $z$ (the matrix of derivatives)\n",
    "\n",
    "The derivative $d_{\\textrm{cost}, z}$ is a $1 \\times K$ vector where\n",
    "$$\n",
    "d_{\\textrm{cost}, z}[i] = \\frac{\\partial L}{\\partial z_i}\n",
    "$$\n",
    "\n",
    "**Given a one-hot-label label vector $y$ with $y_j=1$, show that:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i} = - \\delta_{i, j} + \\frac{1}{\\sum_{a=1}^k e^{z_a}} \\cdot e^{z_i} =  - \\delta_{i, j} + \\textrm{softmax}(z)_i\n",
    "$$\n",
    "where $\\delta_{i,j} =1$ if $i=j$ and zero otherwise.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Implementation\n",
    "You need to complete the following methods\n",
    "- predict - return class predictions for all data points given \n",
    "- score - return mean accuracy of model on given data  \n",
    "- cost_grad - compute average cross entropy cost and gradient of this cost for all parameters\n",
    "- fit - given data run mini batch stochastic gradient descent to learn a good set of parameters that fit the data well, uses a validation set to test current model after each epoch and stores the best weights found (based on accuracy on the validation set).\n",
    "\n",
    "We have included a cost/gradient checker you can run by running **net_classifier.py**.\n",
    "This test is not exhaustive so make your own if need be.\n",
    "\n",
    "When you have completed the exercise run **net_test.py** to test your algorithm on the MNIST digits. You should be able to get above 96 percentage accuracy both in sample and on the test set (results will fluctuate due to randomness in initialization and mini-batches).\n",
    "The code exports plots for your report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Data Set\n",
    "The MNIST digits data set consists of handwritten digits (0 to 9) and the classification task is to recognize the digit. The following piece of code visualizes the first 10 examples in the MNIST data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "Xm, ym = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "fig = plt.figure()\n",
    "axes=[]\n",
    "for i in range(0,10):\n",
    "    image = Xm[i,:]\n",
    "    image = np.array(image, dtype='float')\n",
    "    axes.append(fig.add_subplot(2,5,i+1))\n",
    "    axes[-1].set_axis_off()\n",
    "    pixels = image.reshape((28, 28))\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "Make a report with two sections\n",
    "\n",
    "- Add a section called \"Part I: Derivative\" that includes your derivation of the derivative of the loss function\n",
    "- Add a section called \"Part II: Implementation and test\" that includes the code snippet for your forward pass and for your backward pass - Explain if anything did not work.  Include the train and validation data plots of loss and accuracy generated by net_test.py and comment on anything that looks unusual.   \n",
    "\n",
    "## Uploading to Brightspace\n",
    "Upload the file net_classifier.py \n",
    "\n",
    "Upload one pdf with the report to Brightspace together with net_classifier.py file\n",
    "\n",
    "**Ensure you upload the pdf separately!**\n",
    "\n",
    "**Remember to put your names and student ids inside the pdf report!**\n",
    "\n",
    "**The PDF should be at most 5 pages!**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
