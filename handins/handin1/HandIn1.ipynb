{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand in 1 - 2022 Machine Learning Class\n",
    "For the first hand in you will implement Logistic Regression and Softmax Regression for classification.\n",
    "The descriptions below describe what you are meant to do and hand in. \n",
    "\n",
    "**Start Early, Use The Study Cafe, Check Your Shapes, try and deal with numerial issues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Implementing Logistic Regression\n",
    "In this exercise you must implement logistic regression and test it on text classification.\n",
    "We have provided starter code in the file **logistic_regression.py**. \n",
    "Here you should complete the following methods to implement a Logistic Regression Classifier. \n",
    "\n",
    "* logistic \n",
    "* predict \n",
    "* score\n",
    "* cost_grad\n",
    "* fit\n",
    "\n",
    "where *predict, score, cost_grad, fit* are class methods of the classifier you must implement.\n",
    "The interface for each function is described in the file. \n",
    "\n",
    "All needed equations can be found in the slides.\n",
    "\n",
    "You can test your implementation by running *python logistic_regression.py*. \n",
    "This is a small non-exhaustive test. You should consider writing your own test cases.  \n",
    "\n",
    "Cost and gradient computations sometimes suffers from numerical issues if we are not careful. Exponentation of large numbers and log of small numbers can lead to numerical issues. It is possible to implement the algorithm to be more numerically stable if you do not compute numbers you do not need. The algorithm should work well enough even if you are not so careful and it is not a requirement for passing the hand in to make the ultimate numerically stable algorithm.\n",
    "\n",
    "### Applying Logistic Regression \n",
    "You will test your implementation on a real-life data set obtained from the danish central business register (CVR). The data set consists of text descriptions of the purpose of a company, such as:\n",
    "\n",
    "\"Selskabets form√•l er at drive restaurant og dertil knyttet virksomhed.\"\n",
    "\n",
    "together with a label indicating the so-called industry code, e.g. 561010 is the code for \"Restauranter\" and 620100 is the code for \"Computerprogrammering\".\n",
    "\n",
    "We have already written code that transforms a text description into a feature vector, so you do not need to worry about this translation. We will see later in the course how to do so. In this exercise, you will train a logistic regression classifier to classify descriptions of company purposes into the two classes \"Restauranter\" and \"Computerprogrammering\". If you want to have a look at the data, you can unzip \"branchekoder_formal.gzip\" and open it as a text file (keep the .gzip file in the directory). We are only extracting companies falling in the two categories 561010 and 620100.\n",
    "\n",
    "**Run python logistic_test.py** and see your in-sample and test accuracy on text classification of industry codes (real data)\n",
    "\n",
    "The code automatically saves the generated plot  to include in your report. With a correct implementation and setting of learning rate, batch_size, epochs **you should get above 95 percent test accuracy.**\n",
    "\n",
    "\n",
    "\n",
    "### Report\n",
    "Add a section called \"PART I: Logistic Regression\" with subsections \"Code\" and \"Theory\" to your report. In the code subsection you should have the following subsubsections\n",
    "\n",
    "* Summary and Results: \n",
    " Include the plot generated by logistic_test and include the in-sample and test accuracy you achieve.\n",
    " Add at most two lines explaining the plot(s) and comment anything you believe sticks out.\n",
    " Explain if anything does not work.\n",
    "* Actual Code: Include in your handin code snippets **cost_grad** and **fit** (using for instance verbatim environment in latex)\n",
    "\n",
    "Furthermore you must answer the following three theoretical questions\n",
    "\n",
    "### Theoretical Questions\n",
    "\n",
    "1. What is the running time of your mini-batch gradient descent algorithm?\n",
    "  \n",
    "  The parameters:\n",
    "  * **n**: number of training samples\n",
    "  * **d**: dimensionality of training samples\n",
    "  * **epochs**: number of epochs run\n",
    "  * **mini_batch_size**: batch_size for mini_batch_gradient_descent\n",
    "  \n",
    "  Write both the time to compute the cost and the gradient for log_cost\n",
    "  You can assume that multiplying an $a \\times b$ matrix with a $b \\times c$ matrix takes $O(abc)$ time.\n",
    "\n",
    "\n",
    "2. Sanity Check:\n",
    "\n",
    "Assume you are using Logistic Regression for classifying images of cats and dogs.\n",
    "What happens if we randomly permute the pixels in each image (with the same permutation) before we train the classifier? Will we get a classifier that is better, worse, or the same than if we used the raw data? Give a short explanation (at most three sentences). \n",
    "  HINT: The location of pixels relative to each other seem to hold some kind of information. Does a random permutation of all pixels position affect this locality? Does the model we use exploit pixel locality? \n",
    "\n",
    "3. Linearly Separable Data:\n",
    "\n",
    "If the data is linearly separable, what happens\n",
    "to weights when we implement logistic regression with gradient\n",
    "descent? That is, how do the weights that minimize the negative log likelihood look like?\n",
    "You may assume that we have full precision (that is, ignore floating point errors) and we can run gradient descent as long as we want (i.e. what happens with the weights in the limit). \n",
    "\n",
    "Give a short explanation for your answer. You may include math if it helps (at most 5 lines).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "\n",
    "### Implementing Softmax\n",
    "In \"*softmax.py*\" you must complete the implementation of \n",
    "\n",
    "* logistic\n",
    "* predict\n",
    "* score\n",
    "* cost_grad\n",
    "* fit\n",
    "\n",
    "The interface for each function is described in the file. All needed equations can be found in the slides and in the softmax notebook.\n",
    "Note that we have added a helper function **def one_in_k_encoding(vec, k)** that encodes a vector og length $n$ of integer labels with class labels in $0,\\dots, k-1$ to a $n \\times k$ matrix of one-in-k encoded labels.\n",
    "\n",
    "\n",
    "You can test your implementation by running \"*python softmax.py*\". \n",
    "This is a small non-exhaustive test. You should consider writing your own test cases.\n",
    "\n",
    "As for Logistic Regression, softmax sometimes suffers from numerical issues if we are not careful. Exponentation of large numbers and log of small numbers can lead to numerical issues. It is possible to implement the algorithm to be quite numerically stable if you use the trick provided in the note and ensure you do not take log to numbers. You should do that for this handin.\n",
    "\n",
    "### Applying Softmax\n",
    "You will be testing your implementation on two data sets. The first is the wine data set that we have seen and tested on in week one. The second data set is the MNIST data set. It consists of images of hand-written digits. The goal is to predict/recognize the written digit, i.e. the labels are 0,1,...,9. The MNIST digits are represented as feature vectors with one coordinate per pixel in the image. The ordering of the pixels in the feature vector correspond to taking the pixels of the image and putting them row-by-row after each other.\n",
    "\n",
    "- Run **python softmax_test.py -wine** to test your implementation on the wine data set.\n",
    "The built in python implementation we tested in week one got above 90 percent test accuracy, so your implemenation should so as well.\n",
    "\n",
    "- Run **python softmax_test.py -show_digits** show a small subset of the data set of MNIST digits, a data set for Optical Character Recognition \n",
    "- Run **python softmax_test.py -digits** to run you classifier on MNIST digits - the generated plot is automatically saved\n",
    "- Run **python softmax_test.py -visualize** to visualize the classifier trained on MNIST digits - the generated plot is automatically saved\n",
    "\n",
    "You can tune the epochs, mini_batch_size, and initial learning rate from the command line i.e.\n",
    "\n",
    "**python softmax_test.py -show_digits -epochs 100 -lr 0.42 -bs 666**\n",
    "\n",
    "but the provided values should work well enough.\n",
    "\n",
    "### Report\n",
    "Add a section \"Part II: Softmax\" with subsections \"code\" and \"theory\" to your report. \n",
    "In the \"code\" subsection **you should do the same 2 points as you did for logistic regression**.\n",
    "\n",
    "Include the plots generated by softmax_test and remember to include the in sample and test accuracy achieved.\n",
    "\n",
    "There is a single theory question specified in the next section. \n",
    "\n",
    "\n",
    "### Theoretical Question(s):\n",
    "Assume that you use your softmax implementation on a problem with $K$ classes with n,d, epochs and batch_size defined as for logistic_regression.\n",
    "* What is the running time of your softmax implementation i.e how long does your implementation of cost_grad take to compute the cost and the gradient.\n",
    "\n",
    "\n",
    "# Uploading to Brightspace\n",
    "Make a zip archive of the two code files **logistic.py and softmax.py**\n",
    "\n",
    "Upload one pdf with the report to brightspace together with the zip file.\n",
    "\n",
    "**Ensure you upload the pdf separately!**\n",
    "\n",
    "**Remember to put your names and student ids inside the pdf report!**\n",
    "\n",
    "**The PDF should be at the most 5 pages!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
